{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./src/datasets.json','r') as f:\n",
    "    datasets = json.load(f)\n",
    "    \n",
    "# datasets=datasets['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup ={\n",
    "}\n",
    "rlookup={\n",
    "}\n",
    "def read_corpus(datasets, tokens_only=False):\n",
    "    for index,dataset in enumerate(datasets):\n",
    "            tokens = gensim.utils.simple_preprocess(dataset['resource']['name'] + \" \" + dataset['resource']['description'])\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                lookup[dataset['resource']['id']] = index\n",
    "                rlookup[index] = dataset['resource']['id']\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [index])\n",
    "\n",
    "train_corpus = list(read_corpus(datasets))\n",
    "# test_corpus = list(read_corpus(lee_test_file, tokens_only=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "model.build_vocab(train_corpus)\n",
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.24113488,  1.4916537 ,  0.7330141 , -0.85479075, -0.46601725,\n",
       "        0.284504  ,  0.48587903, -1.67684   , -0.5447132 ,  0.07716218,\n",
       "        0.24809162, -0.60581654, -0.06841498,  0.25660345,  0.93125534,\n",
       "        0.8796326 , -1.1321143 , -0.6407086 ,  0.14011076,  0.21565337,\n",
       "       -0.27137345, -0.58206457,  0.38225177,  0.31509495,  0.14357023,\n",
       "        0.09116443, -0.46132284,  0.20667283, -0.5688424 , -0.3601245 ,\n",
       "        0.66578853, -0.36837518, -0.50423753,  0.51635844,  0.00209239,\n",
       "        0.10683946, -0.37305444, -0.13016497, -0.17788894,  0.1438394 ,\n",
       "        0.15878563,  0.86166734, -0.5937143 , -0.03547985,  0.4861874 ,\n",
       "       -1.4343865 , -0.18082707,  1.5437032 ,  0.02547218, -0.22775139],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = model.infer_vector(train_corpus[0].words)\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fuzi-xuv4'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0]['resource']['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-c1eb57e74d62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdocForId\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ic3t-wcy2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-76-aab4e36ddb29>\u001b[0m in \u001b[0;36mdocForId\u001b[0;34m(did)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdocForId\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_corpus\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdid\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdocForId\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ic3t-wcy2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "docForId('ic3t-wcy2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23, 4, 5, 234, 5]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [23,4,5]\n",
    "b = [234,5]\n",
    "a.extend(b)\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for datasetid in lookup.keys():\n",
    "    inferred_vector = model.infer_vector(train_corpus[lookup[datasetid]].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=30)\n",
    "    sims = [ {'dataset': rlookup[sim[0]], 'similarity': sim[1]} for sim in sims]\n",
    "    results[datasetid] = sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('public/similarity_metrics.json', 'w') as f:\n",
    "    json.dump(results,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'dataset': 'jign-uhe6', 'similarity': 0.6124996542930603},\n",
       " {'dataset': 'shpd-5q9m', 'similarity': 0.5990572571754456},\n",
       " {'dataset': 'i6bk-bwyv', 'similarity': 0.5953792333602905},\n",
       " {'dataset': 'i4kb-6ab6', 'similarity': 0.590259313583374},\n",
       " {'dataset': 'rgy2-tti8', 'similarity': 0.587500274181366},\n",
       " {'dataset': '2mhq-um7h', 'similarity': 0.5857139825820923},\n",
       " {'dataset': 'fx4z-5xg2', 'similarity': 0.5824402570724487},\n",
       " {'dataset': '799n-b76v', 'similarity': 0.5821795463562012},\n",
       " {'dataset': '9z9b-6hvk', 'similarity': 0.5805541276931763},\n",
       " {'dataset': 'c2g8-ercv', 'similarity': 0.579156756401062}]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['jign-uhe6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'dataset': 'jign-uhe6', 'similarity': 0.6124996542930603},\n",
       " {'dataset': 'shpd-5q9m', 'similarity': 0.5990572571754456},\n",
       " {'dataset': 'i6bk-bwyv', 'similarity': 0.5953792333602905},\n",
       " {'dataset': 'i4kb-6ab6', 'similarity': 0.590259313583374},\n",
       " {'dataset': 'rgy2-tti8', 'similarity': 0.587500274181366},\n",
       " {'dataset': '2mhq-um7h', 'similarity': 0.5857139825820923},\n",
       " {'dataset': 'fx4z-5xg2', 'similarity': 0.5824402570724487},\n",
       " {'dataset': '799n-b76v', 'similarity': 0.5821795463562012},\n",
       " {'dataset': '9z9b-6hvk', 'similarity': 0.5805541276931763},\n",
       " {'dataset': 'c2g8-ercv', 'similarity': 0.579156756401062}]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['mnz3-dyi8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = requests.get(\"http://api.us.socrata.com/api/catalog/v1/domains\").json()['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_dataset_list_chunk(domain, page,chunk_size):\n",
    "    offset= page*chunk_size\n",
    "    limit = chunk_size\n",
    "    url='https://api.us.socrata.com/api/catalog/v1?domains={domain}&search_context={domain}&offset={offset}&limit={limit}'\n",
    "    return requests.get(url.format(domain=domain, offset=offset,limit=limit)).json()['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size=100\n",
    "\n",
    "results={}\n",
    "for domain in domains:\n",
    "    datasets = []\n",
    "    print('doing ',domain['domain'])\n",
    "    total = domain['count']\n",
    "    for i in range(total//chunk_size +1): \n",
    "        datasets.extend(get_dataset_list_chunk(domain['domain'], 0 ,100))\n",
    "    results[domain['domain']]=datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_portal_datasets.json', 'w') as f:\n",
    "    json.dump(results,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_portal_datasets.json') as f:\n",
    "    results = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All dataset document modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_lookup={}\n",
    "reverse_all_data_lookup={}\n",
    "index = 0\n",
    "\n",
    "def read_corpus():\n",
    "    for domain in results.keys():\n",
    "        for dataset in results[domain]:\n",
    "            dataset_id = dataset['resource']['id']\n",
    "            key = \"{domain}_{did}\".format(domain=domain, did=dataset_id)\n",
    "            all_data_lookup[key] = index \n",
    "            reverse_all_data_lookup[index] = key\n",
    "            description = dataset['resource']['description']\n",
    "            tokens = gensim.utils.simple_preprocess(dataset['resource']['name'] + \" \" + dataset['resource']['description'])\n",
    "            yield gensim.models.doc2vec.TaggedDocument(tokens, [index])\n",
    "train_corpus = list(read_corpus())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "all_model.build_vocab(train_corpus)\n",
    "all_model.train(train_corpus, total_examples=all_model.corpus_count, epochs=all_model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
