{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Consolidation\n",
    "\n",
    "The aim of this notebook is to find groupings of column_names that refer to the same entity across different datasets. The approach we have taken is:\n",
    "1. Extract a list of unique column names and their frequency of occurrence\n",
    "2. Calculate a the similarity between every two column names, based on Levenshtein distance, stored in a matrix\n",
    "3. Run k-means clustering on the column names, using the similarity of a column name to every other column names (each row in the similarity matrix) as features, and using the column name frequency as weights\n",
    "4. Identify clusters with low average distance from the cluster centroids and inspect these clusters as groups of column names that refer to the same entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\julia\\source\\envs\\fuzzy\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\julia\\source\\envs\\fuzzy\\lib\\site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\julia\\source\\envs\\fuzzy\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\julia\\source\\envs\\fuzzy\\lib\\site-packages (from pandas) (2020.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\julia\\source\\envs\\fuzzy\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\julia\\source\\envs\\fuzzy\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fuzzywuzzy in c:\\users\\julia\\source\\envs\\fuzzy\\lib\\site-packages (0.18.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\julia\\source\\envs\\fuzzy\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\n",
      "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\julia\\source\\envs\\fuzzy\\lib\\site-packages (from python-Levenshtein) (49.2.1)\n",
      "Using legacy 'setup.py install' for python-Levenshtein, since package 'wheel' is not installed.\n",
      "Installing collected packages: python-Levenshtein\n",
      "    Running setup.py install for python-Levenshtein: started\n",
      "    Running setup.py install for python-Levenshtein: finished with status 'done'\n",
      "Successfully installed python-Levenshtein-0.12.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\julia\\source\\envs\\fuzzy\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.1-cp39-cp39-win_amd64.whl (6.9 MB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\julia\\source\\envs\\fuzzy\\lib\\site-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "Collecting scipy>=0.19.1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\julia\\source\\envs\\fuzzy\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading scipy-1.6.2-cp39-cp39-win_amd64.whl (32.7 MB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Using legacy 'setup.py install' for sklearn, since package 'wheel' is not installed.\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, sklearn\n",
      "    Running setup.py install for sklearn: started\n",
      "    Running setup.py install for sklearn: finished with status 'done'\n",
      "Successfully installed joblib-1.0.1 scikit-learn-0.24.1 scipy-1.6.2 sklearn-0.0 threadpoolctl-2.1.0\n"
     ]
    }
   ],
   "source": [
    "## Install dependencies\n",
    "!pip install pandas\n",
    "!pip install fuzzywuzzy\n",
    "!pip install python-Levenshtein\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FuzzMatrix:\n",
    "    \"\"\"\n",
    "    A wrapper around a similarity matrix given a list of terms.\n",
    "    The underlying datastructure is a numpy matrix of dimensions len(terms) x len(terms)\n",
    "    \n",
    "    Scoring options:\n",
    "     - fuzz.ratio (default)\n",
    "     - fuzz.partial_ratio\n",
    "     - fuzz.token_sort_ratio\n",
    "     - fuzz.token_set_ratio\n",
    "    \n",
    "    See this link for descriptions of the different distance metrics:\n",
    "    https://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n",
    "    \"\"\"\n",
    "    def __init__(self, terms, scorer=fuzz.ratio, matrix=None):\n",
    "        \"\"\"\n",
    "        Create a FuzzyMatrix object from terms\n",
    "        \"\"\"\n",
    "        self.terms = terms\n",
    "        self.scorer = scorer\n",
    "        self.matrix = None\n",
    "        self.dict = {}\n",
    "        for i in range(len(terms)):\n",
    "            self.dict[terms[i]] = i\n",
    "        \n",
    "        if matrix == None:\n",
    "            self.calc_matrix()\n",
    "        else:\n",
    "            self.matrix = matrix\n",
    "    \n",
    "    def get_term(self, index: int):\n",
    "        if index >= len(self.terms):\n",
    "            return None\n",
    "        return self.terms[index]\n",
    "    \n",
    "    def score(self, term1, term2):\n",
    "        if term1 not in self.dict or term2 not in self.dict:\n",
    "            return None\n",
    "        \n",
    "        i = self.dict[term1]\n",
    "        j = self.dict[term2]\n",
    "        return self.matrix[i][j]\n",
    "    \n",
    "    def calc_matrix(self):\n",
    "        \"\"\"\n",
    "        Calculate the similarity matrix using the similarity matrix defined in self.scorer\n",
    "        \"\"\"\n",
    "        print(\"Building similarity matrix...\", flush=True)\n",
    "        size = len(self.terms)\n",
    "        self.matrix = np.empty((size, size))\n",
    "        for i in range(size):\n",
    "            for j in range(i, size):\n",
    "                similarity_score = self.scorer(self.terms[i], self.terms[j])\n",
    "                self.matrix[i][j] = similarity_score\n",
    "                self.matrix[j][i] = similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"\n",
    "    Wrapper around k-means clustering and the similarity matrix FuzzMatrix\n",
    "    \"\"\"\n",
    "    def __init__(self, terms, frequencies, scorer=fuzz.ratio):\n",
    "        \"\"\"\n",
    "        terms         : 1-d array-like list of unique terms to cluster\n",
    "        frequencies   : 1-d array-like list of frequencies/number of observations of each column name\n",
    "                        Frequencies will be used as the weights in K-means clustering\n",
    "        \n",
    "        terms and frequencies must be the same length\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        assert len(terms) == len(frequencies), \"terms and frequencies must be the same length\"\n",
    "        self.frequencies = frequencies\n",
    "        self.fuzz_matrix = FuzzMatrix(terms, scorer)\n",
    "        self.kmeans = None\n",
    "        self.clusters = {}\n",
    "        self.distances = []\n",
    "        self.sorted_distances = []\n",
    "    \n",
    "    def k_means(self, num_clusters: int = 100):\n",
    "        print(\"Running K-means clustering on {} terms with {} clusters...\".format(len(self.fuzz_matrix.terms), num_clusters), flush=True)\n",
    "        self.kmeans = KMeans(num_clusters).fit(self.fuzz_matrix.matrix, sample_weight=self.frequencies)\n",
    "        \n",
    "        # sort each terms into clusters -- key: label, value: [list of terms represented by index]\n",
    "        self.clusters = {}\n",
    "        for i in range(len(self.kmeans.labels_)):\n",
    "            label = self.kmeans.labels_[i]\n",
    "            if label not in self.clusters:\n",
    "                self.clusters[label] = [i]\n",
    "            else:\n",
    "                self.clusters[label].append(i)\n",
    "                \n",
    "        # calculate mean distance for each centroid\n",
    "        self.distances = self.calc_mean_distances(self.kmeans.labels_, self.kmeans.cluster_centers_, self.fuzz_matrix.matrix)\n",
    "        self.sorted_distances = sorted([(i, self.distances[i]) for i in range(len(self.distances))], key=lambda x : x[1])\n",
    "        \n",
    "        print(\"Finished running k-means and computing mean cluster spread\", flush=True)\n",
    "            \n",
    "    def calc_mean_distances(self, labels, centroids, data):\n",
    "        \"\"\"\n",
    "        Calculate the average distance for each centroid and return a list of tuples (cluster_index, avg distance)\n",
    "        \n",
    "        Returns a list of length len(centroids)\n",
    "        \"\"\"\n",
    "        distances = [0 for n in range(len(centroids))]\n",
    "        counts = [0 for n in range(len(centroids))]\n",
    "        \n",
    "        \n",
    "        for i in range(len(data)):\n",
    "            row = data[i]\n",
    "            label = labels[i]\n",
    "            centroid = centroids[label]\n",
    "            dist = np.linalg.norm(row - centroid)\n",
    "            distances[label] += dist\n",
    "            counts[label] += 1\n",
    "        \n",
    "\n",
    "        return np.array(distances) / np.array(counts)\n",
    "        \n",
    "    def view_clusters(self, top=10):\n",
    "        \"\"\"\n",
    "        Print out the clusters with the lowest distance scores\n",
    "        \"\"\"\n",
    "        for i in range(top):\n",
    "            label = self.sorted_distances[i][0]\n",
    "            distance = self.sorted_distances[i][1]\n",
    "            cluster = self.clusters[label]\n",
    "            \n",
    "            print(\"Cluster\", i + 1)\n",
    "            print(\"  Mean distance:\", distance)\n",
    "            print(\"  Terms:\", [self.fuzz_matrix.get_term(x) for x in cluster])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code to run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import csv file\n",
    "csv = \"columns_doe.csv\"\n",
    "df = pd.read_csv(csv)\n",
    "\n",
    "# Count occurence of each column name\n",
    "df_text = df.loc[df[\"columns_datatype\"] == \"Text\"]\n",
    "counts = df_text[\"column_name\"].value_counts()\n",
    "\n",
    "unique_column_names = counts.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Levenshtein distance K-means model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building similarity matrix...\n",
      "Running K-means clustering on 3499 terms with 100 clusters...\n",
      "Finished running k-means and computing mean cluster spread\n"
     ]
    }
   ],
   "source": [
    "# Create and run k-means model\n",
    "model = Model(unique_column_names, counts.tolist())\n",
    "model.k_means(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View top X clusters with lowest mean distance to centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d672c3abdefc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview_clusters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.view_clusters(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               term1 freq1         term2 freq2 score\n",
      "11       School Name   186  School Name      1    96\n",
      "86       Grade Level    51    GradeLevel     6    95\n",
      "6        School Name   186    SchoolName     8    95\n",
      "16          Category   180     Category      6    94\n",
      "87       Grade Level    51   Grade level     1    91\n",
      "8        School Name   186   School name     3    91\n",
      "14       School Name   186  ToSchoolName     1    87\n",
      "80           Borough    97       borough     6    86\n",
      "1                DBN   292          DBN      1    86\n",
      "65             Grade   155       Grade 5     1    83\n",
      "57             Grade   155       Grade 9     1    83\n",
      "59             Grade   155       Grade 1     1    83\n",
      "71             Grade   155       Grade K     1    83\n",
      "69             Grade   155       Grade 4     1    83\n",
      "70             Grade   155       Grade 2     1    83\n",
      "72             Grade   155       Grade 3     1    83\n",
      "73             Grade   155       Grade 6     1    83\n",
      "76             Grade   155       Grade 8     1    83\n",
      "55             Grade   155       Grade 7     2    83\n",
      "0                DBN   292            BN     6    80\n",
      "19          Category   180  Sub_Category     1    80\n",
      "20             Grade   155      #Grade 6    10    77\n",
      "38             Grade   155      Grade 5%     3    77\n",
      "41             Grade   155      Grade 9%     3    77\n",
      "42             Grade   155      Grade 6#     3    77\n",
      "45             Grade   155      Grade 8#     3    77\n",
      "46             Grade   155      Grade 1#     3    77\n",
      "47             Grade   155      Grade 5#     3    77\n",
      "49             Grade   155      Grade 4#     3    77\n",
      "50             Grade   155      Grade 8%     3    77\n",
      "52             Grade   155      Grade 7#     3    77\n",
      "53             Grade   155      Grade 9#     3    77\n",
      "21             Grade   155      #Grade 8    10    77\n",
      "61             Grade   155      Grade 11     1    77\n",
      "67             Grade   155      Grade 12     1    77\n",
      "74             Grade   155      Grade 10     1    77\n",
      "79  Mean Scale Score   101    Mean Score     3    77\n",
      "40             Grade   155      Grade 2%     3    77\n",
      "54             Grade   155      Grade 7%     2    77\n",
      "37             Grade   155      Grade 6%     3    77\n",
      "36             Grade   155      Grade 1%     3    77\n",
      "35             Grade   155      Grade 4%     3    77\n",
      "23             Grade   155      #Grade 7     9    77\n",
      "25             Grade   155      #Grade 3     9    77\n",
      "33             Grade   155      Grade 2#     3    77\n",
      "24             Grade   155      #Grade 5     9    77\n",
      "31             Grade   155      #Grade 9     7    77\n",
      "22             Grade   155      #Grade 2     9    77\n",
      "28             Grade   155      #Grade K     9    77\n",
      "27             Grade   155      #Grade 1     9    77\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract column data from json files and write to csv\n",
    "\n",
    "This has already been done and saved to \"columns.csv\" and \"columns_doe.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_department(dataset):\n",
    "    \"\"\"\n",
    "    Given a dataset, return the department\n",
    "    \"\"\"\n",
    "    domain_metadata = dataset[\"classification\"][\"domain_metadata\"]\n",
    "    if domain_metadata == None:\n",
    "        return None\n",
    "    \n",
    "    department = \"Dataset-Information_Agency\"\n",
    "    for d in domain_metadata:\n",
    "        if d[\"key\"] == department:\n",
    "            return d[\"value\"].strip()\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns(dataset):\n",
    "    \"\"\"\n",
    "    Given a dictionary representing a dataset, where each of the json files\n",
    "    in analysis/metadata is considered a list of datasets, retrieve a list of column details,\n",
    "    where each column is represented as a dictionary with the following keys:\n",
    "    {\"column_name\", \"column_field_name\", \"column_type\", \"dataset\", \"department\"}\n",
    "    \"\"\"\n",
    "    department = get_department(dataset)\n",
    "    dataset_name = dataset[\"resource\"][\"name\"]\n",
    "\n",
    "    columns_name = dataset[\"resource\"][\"columns_name\"]\n",
    "    columns_field_name = dataset[\"resource\"][\"columns_field_name\"]\n",
    "    columns_datatype = dataset[\"resource\"][\"columns_datatype\"]\n",
    "    \n",
    "    columns = []\n",
    "    for i in range(len(columns_name)):\n",
    "        column = {}\n",
    "        column[\"column_name\" ] = columns_name[i]\n",
    "        \n",
    "        if i < len(columns_field_name):\n",
    "            column[\"columns_field_name\"] = columns_field_name[i]\n",
    "        else:\n",
    "            column[\"columns_field_name\"] = None\n",
    "        if i < len(columns_datatype):\n",
    "            column[\"columns_datatype\"] = columns_datatype[i]\n",
    "        else:\n",
    "            column[\"columns_datatype\"] = None\n",
    "        \n",
    "        column[\"dataset\"] = dataset_name\n",
    "        column[\"department\"] = department\n",
    "    \n",
    "        columns.append(column)\n",
    "    \n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find all datasets from DOE\n",
    "def extract_column_data(json_filenames, save_as, department=None):\n",
    "    columns = []\n",
    "    for file in json_filenames:\n",
    "        file_json = None\n",
    "        with open(\"metadata\\\\\" + file) as f:\n",
    "            file_json = json.load(f)\n",
    "\n",
    "        for dataset in file_json:\n",
    "            dpt = get_department(dataset)\n",
    "            if department == None:\n",
    "                columns += get_columns(dataset)\n",
    "            elif dpt == department:\n",
    "                columns += get_columns(dataset)\n",
    "\n",
    "    df = pd.DataFrame(columns)\n",
    "    df.to_csv(save_as)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>columns_field_name</th>\n",
       "      <th>columns_datatype</th>\n",
       "      <th>dataset</th>\n",
       "      <th>department</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SAT Writing Avg. Score</td>\n",
       "      <td>sat_writing_avg_score</td>\n",
       "      <td>Text</td>\n",
       "      <td>2012 SAT Results</td>\n",
       "      <td>Department of Education (DOE)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SCHOOL NAME</td>\n",
       "      <td>school_name</td>\n",
       "      <td>Text</td>\n",
       "      <td>2012 SAT Results</td>\n",
       "      <td>Department of Education (DOE)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SAT Math Avg. Score</td>\n",
       "      <td>sat_math_avg_score</td>\n",
       "      <td>Text</td>\n",
       "      <td>2012 SAT Results</td>\n",
       "      <td>Department of Education (DOE)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Num of SAT Test Takers</td>\n",
       "      <td>num_of_sat_test_takers</td>\n",
       "      <td>Text</td>\n",
       "      <td>2012 SAT Results</td>\n",
       "      <td>Department of Education (DOE)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DBN</td>\n",
       "      <td>dbn</td>\n",
       "      <td>Text</td>\n",
       "      <td>2012 SAT Results</td>\n",
       "      <td>Department of Education (DOE)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18148</th>\n",
       "      <td>Description of Code</td>\n",
       "      <td>description_of_code</td>\n",
       "      <td>Text</td>\n",
       "      <td>2014-15 Discharge Reporting By Code - HS</td>\n",
       "      <td>Department of Education (DOE)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18149</th>\n",
       "      <td>Total Enrolled Students</td>\n",
       "      <td>total_enrolled_students</td>\n",
       "      <td>Text</td>\n",
       "      <td>2014-15 Discharge Reporting By Code - HS</td>\n",
       "      <td>Department of Education (DOE)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18150</th>\n",
       "      <td>Count of Students</td>\n",
       "      <td>count_of_students</td>\n",
       "      <td>Text</td>\n",
       "      <td>2014-15 Discharge Reporting By Code - HS</td>\n",
       "      <td>Department of Education (DOE)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18151</th>\n",
       "      <td>Code Type</td>\n",
       "      <td>code_type</td>\n",
       "      <td>Text</td>\n",
       "      <td>2014-15 Discharge Reporting By Code - HS</td>\n",
       "      <td>Department of Education (DOE)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18152</th>\n",
       "      <td>Geographic Unit</td>\n",
       "      <td>geographic_unit</td>\n",
       "      <td>Text</td>\n",
       "      <td>2014-15 Discharge Reporting By Code - HS</td>\n",
       "      <td>Department of Education (DOE)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18153 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   column_name       columns_field_name columns_datatype  \\\n",
       "0       SAT Writing Avg. Score    sat_writing_avg_score             Text   \n",
       "1                  SCHOOL NAME              school_name             Text   \n",
       "2          SAT Math Avg. Score       sat_math_avg_score             Text   \n",
       "3       Num of SAT Test Takers   num_of_sat_test_takers             Text   \n",
       "4                          DBN                      dbn             Text   \n",
       "...                        ...                      ...              ...   \n",
       "18148      Description of Code      description_of_code             Text   \n",
       "18149  Total Enrolled Students  total_enrolled_students             Text   \n",
       "18150        Count of Students        count_of_students             Text   \n",
       "18151                Code Type                code_type             Text   \n",
       "18152          Geographic Unit          geographic_unit             Text   \n",
       "\n",
       "                                        dataset                     department  \n",
       "0                              2012 SAT Results  Department of Education (DOE)  \n",
       "1                              2012 SAT Results  Department of Education (DOE)  \n",
       "2                              2012 SAT Results  Department of Education (DOE)  \n",
       "3                              2012 SAT Results  Department of Education (DOE)  \n",
       "4                              2012 SAT Results  Department of Education (DOE)  \n",
       "...                                         ...                            ...  \n",
       "18148  2014-15 Discharge Reporting By Code - HS  Department of Education (DOE)  \n",
       "18149  2014-15 Discharge Reporting By Code - HS  Department of Education (DOE)  \n",
       "18150  2014-15 Discharge Reporting By Code - HS  Department of Education (DOE)  \n",
       "18151  2014-15 Discharge Reporting By Code - HS  Department of Education (DOE)  \n",
       "18152  2014-15 Discharge Reporting By Code - HS  Department of Education (DOE)  \n",
       "\n",
       "[18153 rows x 5 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get list of json filenames in the metadata dir\n",
    "cwd = os.getcwd()\n",
    "metadata_dir = cwd + \"\\metadata\"\n",
    "json_filenames = [x for x in os.listdir(metadata_dir) if x[-4:] == \"json\"]\n",
    "\n",
    "extract_column_data(json_filenames, \"columns_test.csv\", department=\"Department of Education (DOE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find potential matches for top column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnMatchFinder:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    \n",
    "    def find_possible_matches(self, terms1, terms2, threshold=65, scorer=fuzz.ratio):\n",
    "        \"\"\"\n",
    "        terms1, terms2 : Pandas Series where index=column name, value=counts\n",
    "        threshold      : fuzzywuzzy similarity score threshold - only matches above the threshold will be added to the df\n",
    "\n",
    "        Returns a dataframe with the following columns\n",
    "            - term1\n",
    "            - freq1\n",
    "            - term2\n",
    "            - freq2\n",
    "            - score\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(columns = [\"term1\", \"freq1\", \"term2\", \"freq2\", \"score\"])\n",
    "        i = 0\n",
    "        for term1, freq1 in terms1.iteritems():\n",
    "            for term2, freq2 in terms2.iteritems():\n",
    "                score = scorer(term1, term2)\n",
    "                if score == 1:\n",
    "                    continue\n",
    "                elif score > threshold:\n",
    "                    df.loc[i] = [term1, freq1, term2, freq2, score]\n",
    "                    i += 1\n",
    "                \n",
    "        df.sort_values(\"score\", ascending=False, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    def view_column_name_occurences(self, col: str):\n",
    "        \"\"\"\n",
    "        Prints out the data types and datasets for each column name occurence\n",
    "        \"\"\"\n",
    "        if col not in self.data[\"column_name\"].values:\n",
    "            print(\"No columns found with name: \" + col)\n",
    "        else:\n",
    "            col_metadata = self.get_column_name_metadata(col)\n",
    "            print(col_metadata.reset_index(drop=True))\n",
    "        \n",
    "        \n",
    "    def get_column_name_metadata(self, col: str):\n",
    "        data_col = self.data[self.data[\"column_name\"] == col]\n",
    "        return data_col[[\"columns_datatype\", \"dataset\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julia\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# Read previously generated column data from csv\n",
    "csv = \"columns.csv\"\n",
    "df = pd.read_csv(csv)\n",
    "\n",
    "# get counts of occurences of each column name in a pd Series\n",
    "counts = df[\"column_name\"].value_counts()\n",
    "CMF = ColumnMatchFinder(df)\n",
    "\n",
    "# Find potential column matches\n",
    "possible_matches_levenshtein_ratio = CMF.find_possible_matches(counts[:200], counts[200:], scorer=fuzz.ratio)\n",
    "possible_matches_token_sort_ratio = CMF.find_possible_matches(counts[:200], counts[200:], scorer=fuzz.token_sort_ratio)\n",
    "possible_matches_token_set_ratio = CMF.find_possible_matches(counts[:200], counts[200:], scorer=fuzz.token_set_ratio)\n",
    "\n",
    "# # Save results to csv\n",
    "# # token set ratio gives too many false positives\n",
    "# possible_matches_token_sort_ratio.to_csv(\"token_sort_ratio_matches.csv\")\n",
    "# possible_matches_levenshtein_ratio.to_csv(\"levenshtein_ratio_matches.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    columns_datatype                                            dataset\n",
      "0               Text               IT Performance Customer Satisfaction\n",
      "1               Text                                      City Spending\n",
      "2               Text         City Spending by Department 2013 Pie Chart\n",
      "3               Text                        City Spending by Department\n",
      "4               Text  City of Austin Demographics Current Representa...\n",
      "..               ...                                                ...\n",
      "334             Text                            Expenditures By Program\n",
      "335             Text                      Expenditures, County Attorney\n",
      "336             Text           Expenditures, Registration and Elections\n",
      "337             Text               SPMO Department Financials Bar Chart\n",
      "338             Text   Customer Service Department Financials Bar Chart\n",
      "\n",
      "[339 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "CMF.view_column_name_occurences(\"Department Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fuzzy",
   "language": "python",
   "name": "fuzzy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
